# from https://wikidocs.net/157001 KoGPT2 ì±—ë´‡ ë§Œë“¤ê¸°

# import torch
# from transformers import GPT2LMHeadModel

# from transformers import PreTrainedTokenizerFast
# tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>') 
# tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o")

# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')

# text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'
# input_ids = tokenizer.encode(text)
# gen_ids = model.generate(torch.tensor([input_ids]),
#                            max_length=128,
#                            repetition_penalty=2.0,
#                            pad_token_id=tokenizer.pad_token_id,
#                            eos_token_id=tokenizer.eos_token_id,
#                            bos_token_id=tokenizer.bos_token_id,
#                            use_cache=True)
# generated = tokenizer.decode(gen_ids[0,:].tolist())
# print(generated)

import setModelData
import numpy as np
import pandas as pd
import torch
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.core.lightning import LightningModule
from torch.utils.data import DataLoader, Dataset
from transformers.optimization import AdamW, get_cosine_schedule_with_warmup
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel
import re

Q_TKN = "<usr>"
A_TKN = "<sys>"
BOS = '</s>'
EOS = '</s>'
MASK = '<unused0>'
SENT = '<unused1>'
PAD = '<pad>'

koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
            bos_token=BOS, eos_token=EOS, unk_token='<unk>',
            pad_token=PAD, mask_token=MASK) 
model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')

import urllib.request

urllib.request.urlretrieve(
    "https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv",
    filename="data/ChatBotData.csv",
)
Chatbot_Data = pd.read_csv("data/ChatBotData.csv")
# Test ìš©ìœ¼ë¡œ 300ê°œ ë°ì´í„°ë§Œ ì²˜ë¦¬í•œë‹¤.
Chatbot_Data = Chatbot_Data[:300]
Chatbot_Data.head()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
train_set = setModelData.ChatbotDataset(Chatbot_Data, max_len=40)
#ìœˆë„ìš° í™˜ê²½ì—ì„œ num_workers ëŠ” ë¬´ì¡°ê±´ 0ìœ¼ë¡œ ì§€ì •, ë¦¬ëˆ…ìŠ¤ì—ì„œëŠ” 2
train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=setModelData.collate_batch,)

model.to(device)
model.train()

learning_rate = 3e-5
criterion = torch.nn.CrossEntropyLoss(reduction="none")
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

epoch = 10
Sneg = -1e18

print ("start")
for epoch in range(epoch):
    for batch_idx, samples in enumerate(train_dataloader):
        optimizer.zero_grad()
        token_ids, mask, label = samples
        out = model(token_ids)
        out = out.logits      #Returns a new tensor with the logit of the elements of input
        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)
        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))
        loss = criterion(mask_out.transpose(2, 1), label)
        # í‰ê·  loss ë§Œë“¤ê¸° avg_loss[0] / avg_loss[1] <- loss ì •ê·œí™”
        avg_loss = loss.sum() / mask.sum()
        avg_loss.backward()
        # í•™ìŠµ ë
        optimizer.step()
print ("end")

import datetime as dt

torch.save(model.state_dict(), 'models/' + dt.datetime.now() + 'kogpt2_state_trained_dict.bin')
torch.save(model, 'models/' + dt.datetime.now() + 'kogpt2_model_trained.bin')

with torch.no_grad():
    while 1:
        q = input("user > ").strip()
        if q == "quit":
            break
        a = ""
        while 1:
            input_ids = torch.LongTensor(koGPT2_TOKENIZER.encode(Q_TKN + q + SENT + sent + A_TKN + a)).unsqueeze(dim=0)
            pred = model(input_ids)
            pred = pred.logits
            gen = koGPT2_TOKENIZER.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().numpy().tolist())[-1]
            if gen == EOS:
                break
            a += gen.replace("â–", " ")
        print("Chatbot > {}".format(a.strip()))